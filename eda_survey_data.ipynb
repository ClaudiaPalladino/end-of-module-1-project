{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb938f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. INSTALL REQUIRED LIBRARIES\n",
    "!pip install pandas prince pyreadstat matplotlib seaborn openpyxl\n",
    "\n",
    "# To run categorical association tests\n",
    "!pip install scipy researchpy\n",
    "\n",
    "\n",
    "# 1. IMPORT LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.stats import chi2_contingency\n",
    "from collections import Counter\n",
    "\n",
    "# 2. LOAD DATA (SAV + METADATA)\n",
    "df, meta = pyreadstat.read_sav(r\"C:\")  # data under embargo \n",
    "\n",
    "# CHECK BASIC INFO\n",
    "print(df.shape)\n",
    "print(df.dtypes.value_counts())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abf36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCLUDE UNNECESSARY VAR\n",
    "# Check variables to know what to remove\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK UNIQUE VALUES FOR EACH VAR\n",
    "for col in df.columns:\n",
    "    print(f'{col.upper()} --> valores √∫nicos:', df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43585cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude unnecessary var for the analysis (D10-D13 are var depending on D9, thus few responses)\n",
    "df.drop([\"status\", \"lastpage\", \"startlanguage\", \"startdate\", \"datestamp\", \"ipaddr\", 'codcom_1', 'codcom_2', 'ampiezza','TITOLOSTUDIORECODE', \"Area\", \"provincia\", \"ampiezzaclasse\", \"D09s\", \"D10\", \"D10s\", \"D11\", \"D11s\", \"D12\", \"D12s\", \"peso\"], axis=1, inplace=True)\n",
    "print(df.columns)  # Show variables remaining after removing unnecessary ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d3072",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After var dropping, the dataset comprised observations and variables:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99343c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT THE YEAR FORM THE DATE \"submitdate\" VARIABLE\n",
    "# Check \"submitdate\" type\n",
    "type(df[\"submitdate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c108b",
   "metadata": {},
   "source": [
    "- The survey was implemented/submited in 2025:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcf148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract survey year from submitdate\n",
    "df[\"year_extracted\"] = pd.to_datetime(df[\"submitdate\"], errors=\"coerce\").dt.year\n",
    "\n",
    "# Check the distinct years found in the dataset\n",
    "df[\"year_extracted\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44eaaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After defining the var \"year_extracted\", var \"submitdate\" becomes unnecessary, thus it is excluded\n",
    "df.drop([\"submitdate\"], axis=1, inplace=True)\n",
    "print(df.columns)  # Show variables remaining after removing unnecessary ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fac0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE PARTICIPANTS' AGE\n",
    "# Calculate the age of participants as difference between year of survey implementation and birth year(ETA)\n",
    "df['age'] = 2025 - df['ETA']\n",
    "df[\"age\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71211a",
   "metadata": {},
   "source": [
    "- There are no outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c450cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE CATEGORICAL VAR WITH A LIST\n",
    "categorical_vars = [\n",
    "    \"GEN\", \"RETA\", \"regione\", 'D01', 'D02',\n",
    "       'D03', 'D04', 'D05', 'D06_1', 'D07', 'D08', 'D09', 'D13', 'D14', 'D15',\n",
    "       'D16', 'D17', 'D18', 'D19', 'D20', 'D22'\n",
    "]\n",
    "\n",
    "for col in categorical_vars:\n",
    "    print(col, \"‚Üí\", df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d9a27",
   "metadata": {},
   "source": [
    "- There are no missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2b7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISSING VALUES OVERVIEW\n",
    "missing = pd.DataFrame({\n",
    "    \"Missing\": df[categorical_vars].isna().sum(),\n",
    "    \"Percent\": df[categorical_vars].isna().mean() * 100\n",
    "})\n",
    "print(\"\\nMissing values:\")\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in each column\n",
    "# Returns True where values are null\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD VALUE LABEL DICTIONARY FROM METADATA\n",
    "\n",
    "variable_to_labels = {}\n",
    "\n",
    "for var, label_key in meta.variable_to_label.items():\n",
    "    # Store label dict if available in metadata\n",
    "    if label_key in meta.value_labels:\n",
    "        variable_to_labels[var] = meta.value_labels[label_key]\n",
    "\n",
    "# Optional: Quick inspection\n",
    "import pprint\n",
    "print(\"\\n Labels successfully mapped for these variables:\\n\")\n",
    "pprint.pprint(list(variable_to_labels.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dict to connect each variable name to its descriptive label (metadata)\n",
    "# Two lists from metadata: meta.column_names ‚Üí list of variable names, meta.column_labels ‚Üí list of descriptive labels;\n",
    "# Pair each name with its label: zip()\n",
    "# Convert pairs into a usable dictionary: dict()\n",
    "variable_names = dict(zip(meta.column_names, meta.column_labels))\n",
    "variable_names[\"D19\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232a897",
   "metadata": {},
   "source": [
    "- VALUE CO-OCCURRENCE\n",
    "- Used to identify common response patterns across variables. Only two observations share the same pattern, indicating high heterogeneity in response behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE CO-OCCURRENCE\n",
    "# Example: most common response patterns across selected variables\n",
    "patterns = df[categorical_vars].astype(str).agg(\"-\".join, axis=1)  # axis=1 ‚Üí apply the function row-wise (across columns)\n",
    "# \"-\".join ‚Üí the function being applied: joins all values in the row into a single string separated by -\n",
    "# agg (short for aggregate) is a method used to apply one or more functions across rows or columns\n",
    "\n",
    "print(\"\\nMost common response combinations:\")\n",
    "print(Counter(patterns).most_common(10))\n",
    "\n",
    "# Contar combinaciones\n",
    "combo_counts = df[categorical_vars].value_counts()\n",
    "\n",
    "# Mostrar los 10 m√°s frecuentes\n",
    "combo_counts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a04e3",
   "metadata": {},
   "source": [
    "- UNIVARIATE ANALSYS OF AGE\n",
    "- The mean and median are very similar, suggesting that the distribution of age values is approximately symmetric. This is further confirmed by the skewness statistic (a measure of how asymmetrically a distribution is shaped around its mean) of -0.096, which indicates that the distribution is nearly symmetric, with only a negligible slight left-tail tendency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c624afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for numerical variables\n",
    "summary = df['age'].describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness_value = ((age - mean)**3).mean() / (std**3)\n",
    "# Skewness > 0 ‚Üí right-skewed (long tail to the right)\n",
    "# Skewness < 0 ‚Üí left-skewed (long tail to the left)\n",
    "# Skewness ‚âà 0 ‚Üí symmetric distribution\n",
    "skewness_value = df['age'].skew()\n",
    "print(\"Skewness:\", skewness_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5fbb8",
   "metadata": {},
   "source": [
    "- Histogram showong the raw counts of age values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISATION FOR CONTINUOUS VAR \"age\": HISTOGRAM\n",
    "sns.histplot(df['age'], bins=100)\n",
    "# Outliers: there are no outliers for var \"age\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d94d80",
   "metadata": {},
   "source": [
    "- The plot shows a histogram of age (density), overlaid with a KDE line representing the estimated distribution, a Gaussian curve fitted to the data, and a dashed line indicating the mean age\n",
    "- This visualization improves on a simple histogram by showing the data‚Äôs shape and smooth trends, allowing comparison with a theoretical distribution, and clearly indicating the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81016d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "age = df['age']\n",
    "\n",
    "# Plot histogram\n",
    "sns.histplot(age, bins=100, stat='density', color='skyblue', edgecolor='black', alpha=0.5)\n",
    "\n",
    "# Overlay KDE (estimated distribution)\n",
    "sns.kdeplot(age, color='orange', linewidth=2, label='Estimated distribution (KDE)')\n",
    "\n",
    "# Overlay Gaussian curve for comparison\n",
    "mu, std = norm.fit(age)\n",
    "x = np.linspace(age.min(), age.max(), 100)\n",
    "plt.plot(x, norm.pdf(x, mu, std), 'r--', linewidth=2, label='Gaussian fit')\n",
    "\n",
    "# Add mean line\n",
    "plt.axvline(age.mean(), color='green', linestyle='dashed', linewidth=2, label=f'Mean: {age.mean():.2f}')\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram with KDE and Gaussian Fit')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09a9a0",
   "metadata": {},
   "source": [
    "- Kolmogorov‚ÄìSmirnov test for normality on \"age\"\n",
    "- p ‚â§ 0.05 ‚Üí the distribution of age significantly differs from a normal distribution\n",
    "- the H0 of normal distribution is rejected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "from scipy.stats import kstest, norm\n",
    "# import numpy as np\n",
    "\n",
    "# Extract the age var\n",
    "age = df[\"age\"]\n",
    "\n",
    "# Standardize age to mean 0, std 1 (K-S test requires this when testing normality)\n",
    "age_standardized = (age - np.mean(age)) / np.std(age, ddof=1)\n",
    "\n",
    "# Perform K-S test for normality\n",
    "ks_stat, p_value = kstest(age_standardized, 'norm')\n",
    "\n",
    "print(\"Kolmogorov‚ÄìSmirnov Test for Normality (Age)\")\n",
    "print(f\"KS Statistic: {ks_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIVARIATE ANALYSIS FOR CATEGORICAL VARIABLES\n",
    "\n",
    "# # labels.py ‚Äî FULL TRANSLATION DICTIONARY + HELPERS\n",
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "\n",
    "\n",
    "def categorical_summary(df, categorical_vars):\n",
    "    \"\"\"\n",
    "    Prints frequency tables with counts, percentages, and labels\n",
    "    for a list of categorical variables.\n",
    "    \n",
    "    Automatically uses translated_labels and variable_labels\n",
    "    from labels.py.\n",
    "    \"\"\"\n",
    "\n",
    "    summary_tables = {}\n",
    "\n",
    "    for col in categorical_vars:\n",
    "        # Calculate frequency and percentage\n",
    "        freq = df[col].value_counts(dropna=False).sort_index()\n",
    "        percent = df[col].value_counts(dropna=False, normalize=True).sort_index() * 100\n",
    "\n",
    "        # Get variable descriptive label\n",
    "        var_label = variable_labels.get(col, col)\n",
    "\n",
    "        # Print header\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"üîπ Variable: {col}  ‚Üí  {var_label}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Map numeric codes to labels using map_codes\n",
    "        labels_series = map_codes(freq.index.to_series(), col)\n",
    "\n",
    "        # Build summary table\n",
    "        summary = pd.DataFrame({\n",
    "            \"Category\": freq.index,\n",
    "            \"Label\": labels_series.values,\n",
    "            \"Count\": freq.values,\n",
    "            \"Percent (%)\": percent.round(2)\n",
    "        })\n",
    "\n",
    "        summary_tables[col] = summary\n",
    "        print(summary.to_string(index=False))\n",
    "\n",
    "    return summary_tables\n",
    "\n",
    "\n",
    "# Run the summary\n",
    "tables = categorical_summary(df, categorical_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e798363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZATION TECHNIQUE OF % DISTRIBUTION OF SEX VAR - PIE CHART\n",
    "\n",
    "# #import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "\n",
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "\n",
    "# Prepare data for 'GEN'\n",
    "# df['GEN'] contains numeric codes (1.0, 2.0, etc.)\n",
    "gen_labels = map_codes(df['GEN'], 'GEN')  # Convert codes to text labels\n",
    "sex_counts = gen_labels.value_counts()    # Count occurrences\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(\n",
    "    sex_counts.values,\n",
    "    labels=sex_counts.index,          # Use translated labels\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=90,\n",
    "    colors=['skyblue', 'lightpink', 'lightgray'],  # add color for 'Prefer not to answer' if exists\n",
    "    textprops={'fontsize': 14, 'weight': 'bold'}  # Increase font size and make bold\n",
    ")\n",
    "plt.title('Sex Distribution (%)', fontsize=16, weight='bold')  # bold title\n",
    "plt.axis('equal')  # Equal aspect ratio ensures the pie is circular\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74185e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIVARIATE ANALYSIS FOR CATEGORICAL VAR - PLOT DISTRIBUTIONS (bar charts)\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "\n",
    "def plot_categorical_var(df, categorical_vars, palette=\"Set2\"):\n",
    "    \"\"\"\n",
    "    Plot bar charts for categorical variables with value labels and percentages.\n",
    "    Automatically uses translated_labels and variable_labels from labels.py.\n",
    "    Works for both numeric codes and already text columns.\n",
    "    \"\"\"\n",
    "\n",
    "    FIG_SIZE = (8, 5)\n",
    "\n",
    "    for col in categorical_vars:\n",
    "        # Get variable descriptive label\n",
    "        var_name = variable_labels.get(col, col)\n",
    "\n",
    "        # Get value labels dictionary\n",
    "        labels_dict = translated_labels.get(col, {})\n",
    "        # Convert numeric keys to float\n",
    "        labels_dict = {float(k): v for k, v in labels_dict.items()}\n",
    "\n",
    "        plt.figure(figsize=FIG_SIZE)\n",
    "        ax = sns.countplot(x=df[col], palette=palette)\n",
    "\n",
    "        # Replace x ticks with category labels safely\n",
    "        new_labels = []\n",
    "        for x in ax.get_xticklabels():\n",
    "            text = x.get_text()\n",
    "            try:\n",
    "                # Try to map as float\n",
    "                new_labels.append(labels_dict.get(float(text), text))\n",
    "            except ValueError:\n",
    "                # Keep original text if not numeric\n",
    "                new_labels.append(text)\n",
    "\n",
    "        ax.set_xticklabels(new_labels, rotation=40, ha=\"right\")\n",
    "\n",
    "        # Add percentages on bars\n",
    "        total = len(df[col])\n",
    "        for p in ax.patches:\n",
    "            height = p.get_height()\n",
    "            if height > 0:\n",
    "                percent = f\"{(height / total) * 100:.1f}%\"\n",
    "                ax.text(\n",
    "                    p.get_x() + p.get_width() / 2,\n",
    "                    height,\n",
    "                    percent,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    fontsize=10\n",
    "                )\n",
    "\n",
    "        # Titles and layout\n",
    "        plt.title(f\"Distribution of {col} ‚Äì {var_name}\", fontsize=14, weight='bold')\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage example\n",
    "plot_categorical_var(df, categorical_vars, palette=\"Pastel1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151cb90",
   "metadata": {},
   "source": [
    "- According to the Chi Squared test, four variables are associated to the sex var:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIVARIATE ANALYSIS: chi2 TEST FOR INDEPENDENCE TO ASSESS POSSIBLE ASSOCIATION BETWEEN SEX AND THE OTHER VARS\n",
    "\n",
    "# import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "\n",
    "def chi2_summary_table(df, target_var, categorical_vars, alpha=0.05, round_digits=3, translate_values=True):\n",
    "    \"\"\"\n",
    "    Compute Chi¬≤ test for one target variable against a list of categorical variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "    target_var : str\n",
    "    categorical_vars : list\n",
    "    alpha : float\n",
    "    round_digits : int\n",
    "    translate_values : bool\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Summary table with Chi¬≤, p-value, degrees of freedom, significance, \n",
    "        and fully translated contingency tables.\n",
    "    \"\"\"\n",
    "    # ‚úÖ Ensure full text display in Jupyter\n",
    "    pd.set_option('display.max_colwidth', None)  # shows full text in DataFrame cells\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for var in categorical_vars:\n",
    "        if var == target_var:\n",
    "            continue\n",
    "\n",
    "        subset = df[[target_var, var]].dropna()\n",
    "\n",
    "        # Numeric contingency table for Chi¬≤\n",
    "        contingency_table_numeric = pd.crosstab(subset[target_var], subset[var])\n",
    "\n",
    "        if contingency_table_numeric.shape[0] < 2 or contingency_table_numeric.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        chi2_stat, p_value, dof, _ = chi2_contingency(contingency_table_numeric)\n",
    "\n",
    "        var_name = variable_labels.get(var, var)\n",
    "        sig = \"YES\" if p_value < alpha else \"NO\"\n",
    "\n",
    "        # Optional translated contingency table\n",
    "        if translate_values:\n",
    "            subset_translated = translate(subset)\n",
    "            contingency_table_display = pd.crosstab(subset_translated[target_var],\n",
    "                                                    subset_translated[var])\n",
    "        else:\n",
    "            contingency_table_display = contingency_table_numeric\n",
    "\n",
    "        contingency_dict = contingency_table_display.to_dict()\n",
    "\n",
    "        summary.append({\n",
    "            \"Variable\": var,\n",
    "            \"Description\": var_name,\n",
    "            \"Chi2\": round(chi2_stat, round_digits),\n",
    "            \"p-value\": round(p_value, round_digits),\n",
    "            \"dof\": dof,\n",
    "            \"Significant\": sig,\n",
    "            \"Contingency\": contingency_dict\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_df = summary_df.sort_values(by=\"p-value\").reset_index(drop=True)\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Example usage in Jupyter:\n",
    "categorical_vars = [\n",
    "    \"GEN\", \"RETA\", \"regione\", 'D01', 'D02',\n",
    "    'D03', 'D04', 'D05', 'D06_1', 'D07', 'D08', 'D09', 'D13', 'D14', 'D15',\n",
    "    'D16', 'D17', 'D18', 'D19', 'D20', 'D22'\n",
    "]\n",
    "\n",
    "chi2_results = chi2_summary_table(df, target_var=\"GEN\", categorical_vars=categorical_vars)\n",
    "\n",
    "from IPython.display import display\n",
    "display(chi2_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  'D22': \"Education level\" by sex\n",
    "# Counts table\n",
    "ct_counts = pd.crosstab(df['D22'], df['GEN'])\n",
    "\n",
    "# Percentages by row\n",
    "ct_percent = pd.crosstab(df['D22'], df['GEN'], normalize='index') * 100\n",
    "\n",
    "print(\"COUNTS:\")\n",
    "print(ct_counts)\n",
    "\n",
    "print(\"\\nPERCENTAGES (% by row):\")\n",
    "print(ct_percent.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18063c1a",
   "metadata": {},
   "source": [
    "- Of those who think climate change is not real, 67% are male, while 33% are female, indicating a higher proportion of males in this group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf70b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIVARIATE ANALYSIS: DISTRIBUTION OF SEX AMONG CATEGORIES OF THE FOUR VARIABLES THAT ARE ASSOCIATED TO SEX\n",
    "# According to the Chi Squared test, four variables are associated to the sex var --> check those associations with plots\n",
    "import matplotlib.pyplot as plt\n",
    "from labels import translated_labels, variable_labels, map_codes, translate  # imports\n",
    "\n",
    "# List of variables to plot\n",
    "variables = ['D01', 'D13', 'D14', 'D22']\n",
    "\n",
    "for var in variables:\n",
    "    # Translate both the variable and GEN for readable labels\n",
    "    df_plot = df[[var, 'GEN']].copy()\n",
    "    df_plot[var] = map_codes(df_plot[var], var)\n",
    "    df_plot['GEN'] = map_codes(df_plot['GEN'], 'GEN')\n",
    "\n",
    "    # Group by variable and sex\n",
    "    counts = df_plot.groupby([var, 'GEN']).size().unstack()\n",
    "\n",
    "    # Calculate proportion in %\n",
    "    counts_percent = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plot stacked bar\n",
    "    ax = counts_percent.plot(kind='bar', stacked=True, figsize=(8,5), colormap='tab20')\n",
    "\n",
    "    # Add percentage labels inside each bar\n",
    "    for i, row in enumerate(counts_percent.values):\n",
    "        cum_sum = 0\n",
    "        for j, val in enumerate(row):\n",
    "            if val > 0:  # avoid labeling zero\n",
    "                ax.text(\n",
    "                    i,                # x position (bar index)\n",
    "                    cum_sum + val/2,  # y position (middle of segment)\n",
    "                    f\"{val:.1f}%\",    # label\n",
    "                    ha='center', va='center', color='white', fontsize=9\n",
    "                )\n",
    "                cum_sum += val  # update cumulative height for next segment\n",
    "\n",
    "    # Titles and layout using descriptive variable names from labels.py\n",
    "    label = variable_labels.get(var, var)  # ‚úÖ use variable_labels (full descriptive variable name from labels.py) instead of meta.column_labels\n",
    "    plt.title(f\"Distribution of Sex by {label}\")\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel(\"Proportion (%)\")\n",
    "    plt.legend(title=\"Sex\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263167a",
   "metadata": {},
   "source": [
    "- There are 12 variables that show a statistical significant association with the outcome var D05:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a6f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-squared test for independence to test possible association between output var D05 and the other vars\n",
    "\n",
    "# import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from labels import translated_labels, variable_labels, map_codes, translate  # Updated imports\n",
    "\n",
    "def chi2_summary_table(df, target_var, categorical_vars, alpha=0.05, round_digits=3, translate_values=True):\n",
    "    \"\"\"\n",
    "    Compute Chi¬≤ test for one target variable against a list of categorical variables.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset containing categorical variables.\n",
    "    target_var : str\n",
    "        The categorical variable of interest (e.g., 'D05').\n",
    "    categorical_vars : list\n",
    "        List of categorical variables to test against the target_var.\n",
    "    alpha : float\n",
    "        Significance level for Chi¬≤ test.\n",
    "    round_digits : int\n",
    "        Decimal rounding for Chi¬≤ statistic and p-value.\n",
    "    translate_values : bool\n",
    "        Whether to translate numeric codes into readable labels using labels.py.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Summary table with Chi¬≤, p-value, degrees of freedom, significance, \n",
    "        and translated contingency tables.\n",
    "    \"\"\"\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for var in categorical_vars:\n",
    "        if var == target_var:\n",
    "            continue\n",
    "\n",
    "        # Drop missing values in either column\n",
    "        subset = df[[target_var, var]].dropna()\n",
    "\n",
    "        # Optional translation for readable contingency table\n",
    "        if translate_values:\n",
    "            subset_translated = translate(subset)\n",
    "        else:\n",
    "            subset_translated = subset\n",
    "\n",
    "        # Build contingency table (use translated values for readability)\n",
    "        contingency_table = pd.crosstab(subset_translated[target_var],\n",
    "                                        subset_translated[var])\n",
    "\n",
    "        # Skip degenerate tables\n",
    "        if contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        # Chi¬≤ is computed on numeric codes (subset) to avoid errors\n",
    "        contingency_numeric = pd.crosstab(subset[target_var], subset[var])\n",
    "        chi2_stat, p_value, dof, _ = chi2_contingency(contingency_numeric)\n",
    "\n",
    "        # Use variable_labels from labels.py for descriptive names\n",
    "        var_name = variable_labels.get(var, var)\n",
    "\n",
    "        # Highlight significance\n",
    "        sig = \"YES\" if p_value < alpha else \"NO\"\n",
    "\n",
    "        # Optional: convert contingency table to dict for report-friendly format\n",
    "        contingency_dict = contingency_table.to_dict()\n",
    "\n",
    "        summary.append({\n",
    "            \"Variable\": var,\n",
    "            \"Description\": var_name,\n",
    "            \"Chi2\": round(chi2_stat, round_digits),\n",
    "            \"p-value\": round(p_value, round_digits),\n",
    "            \"dof\": dof,\n",
    "            \"Significant\": sig,\n",
    "            \"Contingency\": contingency_dict\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    summary_df = summary_df.sort_values(by=\"p-value\").reset_index(drop=True)  # sorted by significance\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "categorical_vars = [\n",
    "    \"GEN\", \"RETA\", \"regione\",'D01', 'D02',\n",
    "    'D03', 'D04', 'D06_1', 'D07', 'D08', 'D09', 'D13', 'D14', 'D15',\n",
    "    'D16', 'D17', 'D18', 'D19', 'D20', 'D22'\n",
    "]\n",
    "\n",
    "# Run Chi¬≤ summary for target variable 'D05' with labels integration\n",
    "chi2_results = chi2_summary_table(df, target_var=\"D05\", categorical_vars=categorical_vars)\n",
    "display(chi2_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4f8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter significant variables\n",
    "associated_vars = chi2_results[chi2_results['Significant'] == \"YES\"]\n",
    "\n",
    "# Reset index to have a clean sequential index\n",
    "associated_vars = associated_vars.reset_index(drop=True)\n",
    "\n",
    "# Select only report-relevant columns\n",
    "associated_vars = associated_vars[['Variable', 'Description', 'Chi2', 'p-value']]\n",
    "\n",
    "# Display the filtered and cleaned table\n",
    "from IPython.display import display\n",
    "display(associated_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "\n",
    "# Outcome variable\n",
    "outcome_var = 'D05'\n",
    "\n",
    "# Predictor variables\n",
    "predictors = ['D01', 'D02', 'D03', 'D04', 'D06_1', 'D07', 'D09',\n",
    "              'D13', 'D15', 'D19', 'D20', 'regione']\n",
    "\n",
    "# Translate dataframe for human-readable labels\n",
    "df_plot = translate(df[[outcome_var] + predictors])\n",
    "\n",
    "# Function to get ordered categories from translated_labels\n",
    "def get_order(var):\n",
    "    if var in translated_labels:\n",
    "        sorted_items = sorted(translated_labels[var].items())\n",
    "        return [v for k, v in sorted_items]\n",
    "    else:\n",
    "        # For non-Likert variables, just return sorted unique values\n",
    "        return sorted(df_plot[var].dropna().unique())\n",
    "\n",
    "# Define logical colour palette for D05 (Likert scale)\n",
    "likert_palette = {\n",
    "    'Strongly agree': '#2ca02c',  # vivid green\n",
    "    'Agree': '#98df8a',           # light green\n",
    "    'Neutral': '#d3d3d3',         # light grey\n",
    "    'Disagree': '#c5b0d5',        # light purple\n",
    "    'Strongly disagree': '#9467bd' # strong purple\n",
    "}\n",
    "\n",
    "# Loop over predictor variables\n",
    "for var in predictors:\n",
    "    # Get proper category order\n",
    "    outcome_order = get_order(outcome_var)\n",
    "    predictor_order = get_order(var)\n",
    "\n",
    "    # Convert to ordered categorical\n",
    "    df_plot[outcome_var] = pd.Categorical(df_plot[outcome_var],\n",
    "                                          categories=outcome_order, ordered=True)\n",
    "    df_plot[var] = pd.Categorical(df_plot[var],\n",
    "                                  categories=predictor_order, ordered=True)\n",
    "\n",
    "    # Build contingency table (rows=predictor, columns=outcome)\n",
    "    counts = pd.crosstab(df_plot[var], df_plot[outcome_var])\n",
    "\n",
    "    # Compute percentages\n",
    "    counts_percent = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Plot stacked bar\n",
    "    ax = counts_percent.plot(\n",
    "        kind='bar', stacked=True, figsize=(10,6),\n",
    "        color=[likert_palette.get(c, '#cccccc') for c in counts_percent.columns]  # fallback grey\n",
    "    )\n",
    "\n",
    "    # Add percentages inside bars\n",
    "    for i, row in enumerate(counts_percent.values):\n",
    "        cum_sum = 0\n",
    "        for j, val in enumerate(row):\n",
    "            if val > 0:\n",
    "                ax.text(\n",
    "                    i, cum_sum + val/2,\n",
    "                    f\"{val:.1f}%\", \n",
    "                    ha='center', va='center', weight='bold', fontsize=10\n",
    "                )\n",
    "                cum_sum += val\n",
    "\n",
    "    # Titles and axis labels using variable_labels\n",
    "    ax.set_title(f\"Distribution of {variable_labels.get(outcome_var)} by {variable_labels.get(var)}\",\n",
    "                 weight='bold', fontsize=12)\n",
    "    ax.set_xlabel(variable_labels.get(var), weight='bold')\n",
    "    ax.set_ylabel(\"Proportion (%)\", weight='bold')\n",
    "    ax.set_ylim(0, 100)\n",
    "    plt.xticks(rotation=45, weight='bold')\n",
    "\n",
    "    # Move legend to the right\n",
    "    ax.legend(\n",
    "        title=variable_labels.get(outcome_var),\n",
    "        title_fontsize=10,\n",
    "        fontsize=10,\n",
    "        bbox_to_anchor=(1.02, 1),  # outside right\n",
    "        loc='upper left',\n",
    "        borderaxespad=0\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8545c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of D05 across categories of selected predictors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from labels import translated_labels, variable_labels, map_codes, translate  # required labels file\n",
    "\n",
    "# Custom ordinal color gradient for D05 categories:\n",
    "# Adjust hex colors if needed, but these follow your semantics\n",
    "d05_colors = {\n",
    "    \"Strongly agree\": \"#00A600\",   # vivid green\n",
    "    \"Agree\": \"#8CD17D\",            # soft light green\n",
    "    \"Neutral\": \"#D9D9D9\",          # light grey\n",
    "    \"Disagree\": \"#CAB2D6\",         # light purple\n",
    "    \"Strongly disagree\": \"#6A3D9A\" # strong purple\n",
    "}\n",
    "\n",
    "d05_label = variable_labels.get('D05', 'D05')\n",
    "\n",
    "predictors = ['D01', 'D02', 'D03', 'D04', 'D06_1', 'D07', 'D09',\n",
    "              'D13', 'D15', 'D19', 'D20', 'regione']\n",
    "\n",
    "figsize = (10, 6)\n",
    "label_threshold_pct = 2.0\n",
    "\n",
    "for var in predictors:\n",
    "\n",
    "    if var not in df.columns:\n",
    "        print(f\"Warning: {var} not found in dataframe. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    subset = df[[var, 'D05']].dropna()\n",
    "\n",
    "    # Apply label translation\n",
    "    sub_disp = subset.copy()\n",
    "    sub_disp[var] = map_codes(sub_disp[var], var)\n",
    "    sub_disp['D05'] = map_codes(sub_disp['D05'], 'D05')\n",
    "\n",
    "    # Build contingency table\n",
    "    counts = pd.crosstab(sub_disp[var], sub_disp['D05'])\n",
    "\n",
    "    if counts.empty:\n",
    "        continue\n",
    "\n",
    "    # Percent distribution\n",
    "    counts_pct = counts.div(counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "    # Order D05 responses according to the keys in translated_labels['D05']\n",
    "    if 'D05' in translated_labels:\n",
    "        ordered_cols = []\n",
    "        for k in translated_labels['D05'].keys():\n",
    "            lbl = translated_labels['D05'][k]\n",
    "            if lbl in counts_pct.columns:\n",
    "                ordered_cols.append(lbl)\n",
    "        # Add missing ones if any\n",
    "        for c in counts_pct.columns:\n",
    "            if c not in ordered_cols:\n",
    "                ordered_cols.append(c)\n",
    "\n",
    "        counts_pct = counts_pct[ordered_cols]\n",
    "\n",
    "    # Apply color map in correct order\n",
    "    color_list = [d05_colors[c] for c in counts_pct.columns]\n",
    "\n",
    "    # Plot\n",
    "    ax = counts_pct.plot(\n",
    "        kind=\"bar\", \n",
    "        stacked=True, \n",
    "        figsize=figsize,\n",
    "        color=color_list\n",
    "    )\n",
    "\n",
    "    # Label percentages inside segments\n",
    "    for i, row in enumerate(counts_pct.values):\n",
    "        cum = 0\n",
    "        for j, val in enumerate(row):\n",
    "            if val >= label_threshold_pct:\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    cum + val/2,\n",
    "                    f\"{val:.1f}%\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\",\n",
    "                    fontsize=10,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "            cum += val\n",
    "\n",
    "    # Titles and axes\n",
    "    var_label = variable_labels.get(var, var)\n",
    "    # plt.title(f\"Distribution of {d05_label} by {var_label}\")\n",
    "    plt.title(f\"‚ÄúAttitude toward environmental policy‚Äù by {var_label}\")\n",
    "    plt.xlabel(var_label)\n",
    "    plt.ylabel(\"Proportion (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    plt.legend(title=d05_label, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL CRAMER'S V HEATMAP (all categorical variables √ó all categorical variables)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from labels import translated_labels, variable_labels, map_codes, translate\n",
    "\n",
    "# Function to compute Cram√©r's V for two categorical variables\n",
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    if confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
    "        return np.nan  # Skip degenerate tables\n",
    "    chi2_stat, p, dof, _ = chi2_contingency(confusion_matrix)\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    return np.sqrt(chi2_stat / (n * (min(confusion_matrix.shape) - 1)))\n",
    "\n",
    "# Compute a symmetric Cram√©r's V matrix for a list of categorical variables\n",
    "def cramers_v_matrix(df, categorical_vars, variable_names=None):\n",
    "    n = len(categorical_vars)\n",
    "    matrix = pd.DataFrame(np.zeros((n, n)), index=categorical_vars, columns=categorical_vars)\n",
    "\n",
    "    for i, var1 in enumerate(categorical_vars):\n",
    "        for j, var2 in enumerate(categorical_vars):\n",
    "            if i <= j:  # compute upper triangle\n",
    "                v = cramers_v(df[var1], df[var2])\n",
    "                matrix.loc[var1, var2] = v\n",
    "                matrix.loc[var2, var1] = v  # symmetric\n",
    "\n",
    "    # Optionally rename variables using variable_labels\n",
    "    if variable_names:\n",
    "        matrix.rename(index=variable_names, columns=variable_names, inplace=True)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "# Plot the heatmap with discrete legend\n",
    "def plot_cramers_v_heatmap_discrete(df, categorical_vars, variable_names=None, figsize=(14,12)):\n",
    "    # Translate dataframe to readable labels\n",
    "    df_translated = translate(df[categorical_vars])\n",
    "    \n",
    "    # Compute Cram√©r's V matrix\n",
    "    cv_matrix = cramers_v_matrix(df_translated, categorical_vars, variable_names=variable_names)\n",
    "    \n",
    "    # Define discrete categories\n",
    "    bins = [0.0, 0.10, 0.20, 0.40, 0.60, 0.80, 1.0]\n",
    "    labels = [\n",
    "        \"Negligible / Very weak\",\n",
    "        \"Weak\",\n",
    "        \"Moderate\",\n",
    "        \"Relatively strong\",\n",
    "        \"Strong\",\n",
    "        \"Very strong / Almost perfect\"\n",
    "    ]\n",
    "    \n",
    "    # Map values to discrete categories\n",
    "    cv_cat = np.digitize(cv_matrix.fillna(0).values, bins, right=False) - 1\n",
    "    cv_cat = np.clip(cv_cat, 0, len(labels)-1)\n",
    "    \n",
    "    # Discrete colormap: light to strong red\n",
    "    colors = sns.light_palette(\"red\", n_colors=len(labels), reverse=False)\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cv_cat, annot=np.round(cv_matrix.values,2), fmt=\".2f\",\n",
    "                cmap=cmap, cbar=False, square=True,\n",
    "                xticklabels=cv_matrix.columns, yticklabels=cv_matrix.index)\n",
    "    \n",
    "    # Create discrete legend\n",
    "    legend_elements = [Patch(facecolor=cmap(i), edgecolor='k', label=labels[i]) for i in range(len(labels))]\n",
    "    plt.legend(handles=legend_elements, title=\"Association Strength\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.title(\"Cram√©r's V Heatmap for Categorical Variables\", fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cv_matrix\n",
    "\n",
    "# List of categorical variables\n",
    "categorical_vars = [\n",
    "    \"GEN\", \"RETA\", \"regione\", 'D01', 'D02',\n",
    "    'D03', 'D04', 'D05', 'D06_1', 'D07', 'D08', 'D09',\n",
    "    'D13', 'D14', 'D15', 'D16', 'D17', 'D18', 'D19', 'D20', 'D22'\n",
    "]\n",
    "\n",
    "# Compute and plot\n",
    "cv_matrix = plot_cramers_v_heatmap_discrete(df, categorical_vars, variable_names=variable_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a890a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def cramers_v(x, y):\n",
    "    \"\"\"Calculate Cram√©r's V between two categorical vectors.\"\"\"\n",
    "    table = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(table, correction=False)[0]\n",
    "    n = table.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = table.shape\n",
    "    return np.sqrt(phi2 / (min(k - 1, r - 1)))\n",
    "\n",
    "# CREATE THE MATRIX\n",
    "cramers_v_df = pd.DataFrame(index=categorical_vars, columns=categorical_vars, dtype=float)\n",
    "\n",
    "for row in categorical_vars:\n",
    "    for col in categorical_vars:\n",
    "        cramers_v_df.loc[row, col] = cramers_v(df[row], df[col])\n",
    "\n",
    "cramers_v_df\n",
    "\n",
    "def extract_strong_associations(cramers_v_df, threshold=0.20):\n",
    "    results = []\n",
    "\n",
    "    for row in cramers_v_df.index:\n",
    "        for col in cramers_v_df.columns:\n",
    "            if row != col:  # skip diagonal\n",
    "                v = cramers_v_df.loc[row, col]\n",
    "                if v >= threshold:\n",
    "                    results.append((row, col, v))\n",
    "\n",
    "    assoc_df = pd.DataFrame(results, columns=[\"Variable 1\", \"Variable 2\", \"Cram√©r‚Äôs V\"])\n",
    "    assoc_df = assoc_df.sort_values(by=\"Cram√©r‚Äôs V\", ascending=False)\n",
    "\n",
    "    # remove duplicate (A-B and B-A pairs)\n",
    "    assoc_df = assoc_df[assoc_df[\"Variable 1\"] < assoc_df[\"Variable 2\"]]\n",
    "\n",
    "    return assoc_df\n",
    "\n",
    "strong_associations = extract_strong_associations(cramers_v_df, threshold=0.20)\n",
    "strong_associations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57892d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert Cramer matrix to long format\n",
    "assoc = cramers_v_df.stack().reset_index()\n",
    "assoc.columns = [\"Var1\", \"Var2\", \"CramersV\"]\n",
    "\n",
    "# 2. Remove self-correlations and duplicates\n",
    "assoc = assoc[assoc[\"Var1\"] < assoc[\"Var2\"]]\n",
    "\n",
    "# 3. Keep only meaningful associations > 0.20\n",
    "assoc = assoc[assoc[\"CramersV\"] > 0.20]\n",
    "\n",
    "# 4. Add variable descriptions\n",
    "assoc[\"Var1 label\"] = assoc[\"Var1\"].map(variable_names)\n",
    "assoc[\"Var2 label\"] = assoc[\"Var2\"].map(variable_names)\n",
    "\n",
    "# 5. Categorize strength levels\n",
    "def cramers_category(v):\n",
    "    if 0.20 <= v < 0.40: return \"Moderate\"\n",
    "    if 0.40 <= v < 0.60: return \"Relatively strong\"\n",
    "    if 0.60 <= v < 0.80: return \"Strong\"\n",
    "    if v >= 0.80:        return \"Very strong\"\n",
    "\n",
    "assoc[\"Strength\"] = assoc[\"CramersV\"].apply(cramers_category)\n",
    "\n",
    "# 6. Sort by highest association\n",
    "assoc = assoc.sort_values(by=\"CramersV\", ascending=False)\n",
    "\n",
    "# 7. Apply color palette (BuPu shades)\n",
    "def highlight_palette(val):\n",
    "    colors = {\n",
    "        \"Moderate\": \"#c7e9f1\",\n",
    "        \"Relatively strong\": \"#7eb7d6\",\n",
    "        \"Strong\": \"#4675b4\",\n",
    "        \"Very strong\": \"#2b3c8a\"\n",
    "    }\n",
    "    return f\"background-color: {colors.get(val, 'white')}; color: black\"\n",
    "\n",
    "# 8. Display formatted table\n",
    "styled_table = assoc.style.applymap(highlight_palette, subset=[\"Strength\"])\n",
    "styled_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# 1. Compute Cram√©r's V\n",
    "def cramers_v(x, y):\n",
    "    table = pd.crosstab(x, y)\n",
    "    chi2 = chi2_contingency(table, correction=False)[0]\n",
    "    n = table.sum().sum()\n",
    "    phi2 = chi2 / n\n",
    "    r, k = table.shape\n",
    "    return np.sqrt(phi2 / (min(k - 1, r - 1)))\n",
    "\n",
    "# 2. Define your survey blocks\n",
    "env_awareness = [\"D01\", \"D02\", \"D03\", \"D04\", \"D05\"]\n",
    "env_behavior = [\"D06_1\", \"D07\", \"D08\", \"D09\", \"D13\"]\n",
    "civic_engagement = [\"D14\", \"D15\", \"D16\", \"D17\", \"D18\", \"D19\", \"D20\"]\n",
    "demographics = [\"GEN\", \"RETA\", \"regione\", \"D22\"]\n",
    "\n",
    "blocks = {\n",
    "    \"Environmental Awareness and Regulatory Attitudes\": env_awareness,\n",
    "    \"Environmental Behaviors\": env_behavior,\n",
    "    \"Civic Engagement\": civic_engagement,\n",
    "    \"Demographics\": demographics,\n",
    "}\n",
    "\n",
    "# 3. Define thresholds and labels (as requested)\n",
    "thresholds = [0.0, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "labels = [\n",
    "    \"Negligible [0‚Äì0.10[\",\n",
    "    \"Weak [0.10‚Äì0.20[\",\n",
    "    \"Moderate [0.20‚Äì0.40[\",\n",
    "    \"Relatively strong [0.40‚Äì0.60[\",\n",
    "    \"Strong [0.60‚Äì0.80[\",\n",
    "    \"Very strong [0.80‚Äì1.00]\"\n",
    "]\n",
    "\n",
    "# BuPu color palette (6 categories)\n",
    "palette = sns.color_palette(\"BuPu\", len(labels))\n",
    "cmap = ListedColormap(palette)\n",
    "norm = BoundaryNorm(thresholds, cmap.N)\n",
    "\n",
    "# 4. Compute block-level Cram√©r matrices\n",
    "def compute_cramers_matrix(df, variables):\n",
    "    matrix = pd.DataFrame(index=variables, columns=variables, dtype=float)\n",
    "    for v1 in variables:\n",
    "        for v2 in variables:\n",
    "            matrix.loc[v1, v2] = 1.0 if v1 == v2 else cramers_v(df[v1], df[v2])\n",
    "    return matrix\n",
    "\n",
    "# 5. Plot block heatmap (NO continuous legend)\n",
    "def plot_block_heatmap(matrix, title):\n",
    "    plt.figure(figsize=(7, 6))\n",
    "\n",
    "    sns.heatmap(\n",
    "        matrix,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        square=True,\n",
    "        cbar=False,            # No vertical colorbar\n",
    "        annot_kws={\"size\": 9}\n",
    "    )\n",
    "\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Discrete category legend only, no color bar\n",
    "    legend_patches = [\n",
    "        Patch(facecolor=palette[i], label=labels[i])\n",
    "        for i in range(len(labels))\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_patches,\n",
    "        title=\"Association strength\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Generate heatmaps by thematic block\n",
    "for block_name, vars_list in blocks.items():\n",
    "    matrix = compute_cramers_matrix(df, vars_list)\n",
    "    plot_block_heatmap(matrix, f\"Cram√©r‚Äôs V ‚Äî {block_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
